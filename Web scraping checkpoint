{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWAGIOWm9w+DxfnpR65HwY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OuGID1w-7Oy9","executionInfo":{"status":"ok","timestamp":1748855876476,"user_tz":-180,"elapsed":7337,"user":{"displayName":"Filbao","userId":"06534393836955986277"}},"outputId":"2e3eac03-fdc5-4a32-f18e-7e4dd5896526"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"]}],"source":["pip install requests beautifulsoup4\n"]},{"cell_type":"markdown","source":["Write a function to Get and parse html content from a Wikipedia page"],"metadata":{"id":"uilUbf1m8R5v"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","\n","def get_html_content(url):\n","    \"\"\"Fetch and parse HTML content from a Wikipedia page.\"\"\"\n","    response = requests.get(url)\n","    response.raise_for_status()  # Raise an error for bad status\n","    return BeautifulSoup(response.text, 'html.parser')"],"metadata":{"id":"9n-qCkk-8gYa","executionInfo":{"status":"ok","timestamp":1748856133797,"user_tz":-180,"elapsed":7,"user":{"displayName":"Filbao","userId":"06534393836955986277"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Write a function to Extract article title"],"metadata":{"id":"BpPoy6U_82Ba"}},{"cell_type":"code","source":["def extract_title(soup):\n","    \"\"\"Extract the article title from the page.\"\"\"\n","    return soup.find('h1', id='firstHeading').text.strip()"],"metadata":{"id":"QLSV32rw82qB","executionInfo":{"status":"ok","timestamp":1748856184079,"user_tz":-180,"elapsed":4,"user":{"displayName":"Filbao","userId":"06534393836955986277"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Write a function to Extract article text for each paragraph with their respective headings. Map those headings to their respective paragraphs in the dictionary."],"metadata":{"id":"eIPtR8kj86hF"}},{"cell_type":"code","source":["def extract_content_by_headings(soup):\n","    \"\"\"\n","    Extract paragraphs under their respective headings.\n","    Returns a dictionary mapping headings to list of paragraph texts.\n","    \"\"\"\n","    content = {}\n","    current_heading = \"Introduction\"\n","    content[current_heading] = []\n","\n","    content_div = soup.find('div', id='mw-content-text')\n","    for tag in content_div.find_all(['h2', 'h3', 'p'], recursive=True):\n","        if tag.name in ['h2', 'h3']:\n","            heading = tag.text.strip().replace(\"[edit]\", \"\").strip()\n","            current_heading = heading\n","            if current_heading not in content:\n","                content[current_heading] = []\n","        elif tag.name == 'p':\n","            text = tag.get_text(strip=True)\n","            if text:\n","                content[current_heading].append(text)\n","    return content\n"],"metadata":{"id":"tZeeLKyU8984","executionInfo":{"status":"ok","timestamp":1748856283809,"user_tz":-180,"elapsed":10,"user":{"displayName":"Filbao","userId":"06534393836955986277"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Write a function to collect every link that redirects to another Wikipedia page"],"metadata":{"id":"wk77XhVu9TrJ"}},{"cell_type":"code","source":["def extract_internal_links(soup):\n","    \"\"\"\n","    Extract all internal Wikipedia links from the page.\n","    Returns a list of full URLs.\n","    \"\"\"\n","    base_url = \"https://en.wikipedia.org\"\n","    links = set()\n","    for a_tag in soup.select('#mw-content-text a[href^=\"/wiki/\"]'):\n","        href = a_tag.get('href')\n","        if ':' not in href:  # Skip special and file pages\n","            full_url = urljoin(base_url, href)\n","            links.add(full_url)\n","    return list(links)"],"metadata":{"id":"5zljEyKO9Wmb","executionInfo":{"status":"ok","timestamp":1748856330747,"user_tz":-180,"elapsed":41,"user":{"displayName":"Filbao","userId":"06534393836955986277"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Wrap all the previous functions into a single function that takes as parameters a Wikipedia link"],"metadata":{"id":"IKqduulC9edN"}},{"cell_type":"code","source":["def scrape_wikipedia_page(url):\n","    \"\"\"\n","    Consolidated function to scrape a Wikipedia page.\n","    Returns a dictionary with title, content_by_heading, and internal_links.\n","    \"\"\"\n","    soup = get_html_content(url)\n","    title = extract_title(soup)\n","    content = extract_content_by_headings(soup)\n","    links = extract_internal_links(soup)\n","\n","    return {\n","        'title': title,\n","        'content_by_heading': content,\n","        'internal_links': links\n","    }"],"metadata":{"id":"8scFa84z9iCh","executionInfo":{"status":"ok","timestamp":1748856374742,"user_tz":-180,"elapsed":11,"user":{"displayName":"Filbao","userId":"06534393836955986277"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Test the last function on a Wikipedia page of your choice"],"metadata":{"id":"3NxgowHL9pnr"}},{"cell_type":"code","source":["#  TESTING the function on a Wikipedia page\n","if __name__ == \"__main__\":\n","    test_url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n","    result = scrape_wikipedia_page(test_url)\n","\n","    # Display results\n","    print(\"Title:\", result['title'])\n","    print(\"\\nHeadings and content (excerpt):\")\n","    for heading, paragraphs in result['content_by_heading'].items():\n","        print(f\"\\n## {heading}\")\n","        for para in paragraphs[:2]:  # Show only first 2 paragraphs per heading\n","            print(f\"- {para[:100]}...\")  # Print excerpt\n","\n","    print(f\"\\nNumber of internal links found: {len(result['internal_links'])}\")\n","    print(\"Sample links:\", result['internal_links'][:5])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dWLleljs9rnm","executionInfo":{"status":"ok","timestamp":1748856455179,"user_tz":-180,"elapsed":121,"user":{"displayName":"Filbao","userId":"06534393836955986277"}},"outputId":"ac73f293-5272-45c7-aca5-c624f13977f2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Title: Web scraping\n","\n","Headings and content (excerpt):\n","\n","## Introduction\n","- Web scraping,web harvesting, orweb data extractionisdata scrapingused forextracting datafromwebsites...\n","- Scraping a web page involves fetching it and then extracting data from it. Fetching is the downloadi...\n","\n","## History\n","- After thebirth of the World Wide Webin 1989, the first web robot,[2]World Wide Web Wanderer, was cre...\n","- In December 1993, the first crawler-based web search engine,JumpStation, was launched. As there were...\n","\n","## Techniques\n","- Web scraping is the process of automatically mining data or collecting information from the World Wi...\n","\n","## Human copy-and-paste\n","- The simplest form of web scraping is manually copying and pasting data from a web page into a text f...\n","\n","## Text pattern matching\n","- A simple yet powerful approach to extract information from web pages can be based on the UNIXgrepcom...\n","\n","## HTTP programming\n","- Staticanddynamic web pagescan be retrieved by posting HTTP requests to the remote web server usingso...\n","\n","## HTML parsing\n","- Many websites have large collections of pages generated dynamically from an underlying structured so...\n","\n","## DOM parsing\n","- By using a program such asSeleniumorPlaywright, developers can control a web browser such asChromeor...\n","\n","## Vertical aggregation\n","- There are several companies that have developed vertical specific harvesting platforms. These platfo...\n","\n","## Semantic annotation recognizing\n","- The pages being scraped may embracemetadataor semantic markups and annotations, which can be used to...\n","\n","## Computer vision web-page analysis\n","- There are efforts usingmachine learningandcomputer visionthat attempt to identify and extract inform...\n","\n","## AI-powered document understanding\n","- Uses advanced AI to interpret and process web page content contextually, extracting relevant informa...\n","\n","## Legal issues\n","- The legality of web scraping varies across the world. In general, web scraping may be against theter...\n","\n","## United States\n","- In the United States, website owners can use three majorlegal claimsto prevent undesired web scrapin...\n","- U.S. courts have acknowledged that users of \"scrapers\" or \"robots\" may be held liable for committing...\n","\n","## European Union\n","- In February 2006, theDanish Maritime and Commercial Court(Copenhagen) ruled that systematic crawling...\n","- In a February 2010 case complicated by matters of jurisdiction, Ireland's High Court delivered a ver...\n","\n","## Australia\n","- In Australia, theSpam Act 2003outlaws some forms of web harvesting, although this only applies to em...\n","\n","## India\n","- Leaving a few cases dealing with IPR infringement, Indian courts have not expressly ruled on the leg...\n","\n","## Methods to prevent web scraping\n","- The administrator of a website can use various measures to stop or slow a bot. Some techniques inclu...\n","\n","## See also\n","\n","## References\n","\n","Number of internal links found: 131\n","Sample links: ['https://en.wikipedia.org/wiki/Text_corpus', 'https://en.wikipedia.org/wiki/Database', 'https://en.wikipedia.org/wiki/Search_engine_scraping', 'https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web', 'https://en.wikipedia.org/wiki/Fake_news_website']\n"]}]}]}