{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4802,"status":"ok","timestamp":1754642295364,"user":{"displayName":"Filbao","userId":"06534393836955986277"},"user_tz":-180},"id":"KySNjV74zaQr","outputId":"1d3a4108-4bf9-4ca3-9ebb-496c02392c3e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["#Import libraries and download NLTK data\n","import nltk #Imports nltk for natural language processing tasks\n","nltk.download('punkt') #Downloads punkt for splitting text into sentences and words\n","nltk.download('stopwords') #Downloads stopwords to filter common words(e.g., 'the','is')\n","nltk.download('wordnet') #Downloads wordnet for lemmatizing words to their base form\n","from nltk.tokenize import word_tokenize,sent_tokenize #Imports functions to split text into words and sentences\n","from nltk.corpus import stopwords #Imports stopwords to remove common words\n","from nltk.stem import WordNetLemmatizer #Imports Lemmatizer to reduce words to their root form\n","import string #Imports string module for punctuation filtering\n","nltk.download('punkt_tab')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1754642506615,"user":{"displayName":"Filbao","userId":"06534393836955986277"},"user_tz":-180},"id":"_j8x2z1y1Hfy"},"outputs":[],"source":["#Step 1:\n","#Load and preprocess text data\n","def preprocess(sentence):\n","    #Tokenize text into sentences\n","    words = word_tokenize(sentence)\n","    #Tokenize sentences into words\n","    words = [word.lower() for word in words if word.lower() not in stopwords.words('english') and word not in string.punctuation]\n","    lemmatizer=WordNetLemmatizer()\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    return words"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1754642828273,"user":{"displayName":"Filbao","userId":"06534393836955986277"},"user_tz":-180},"id":"76LBypRE1-gG"},"outputs":[],"source":["#Step 2\n","#Create a function to load and preprocess text from a file\n","def load_and_preprocess_text(file_path): #Defines function to load and preprocess text from a file\n","    with open(file_path,'r') as file: #Opens file in read mode,ensures proper closure\n","        data=file.read().replace('\\n','') #Reads entire file,removes newline characters\n","        sentences=data.split('.') #Splits text into sentences at periods\n","        sentences=[s.strip() for s in sentences if s.strip()] #Removes whitespace,filters out empty sentences\n","        corpus=[preprocess(sentence) for sentence in sentences] #Applies preprocess fucntion to each sentence\n","        return sentences,corpus #Returns list of sentences and preprocessed corpus"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1754643037170,"user":{"displayName":"Filbao","userId":"06534393836955986277"},"user_tz":-180},"id":"8cCqNHWJ3Ryl"},"outputs":[],"source":["#Define a fucntion to find the most relevant snetence given in a query\n","\n","#Find the most relevant sentence given a query\n","def get_most_relevant_sentence(query,corpus,sentences):\n","    query=preprocess(query)\n","    max_similarity=0\n","    most_relevant_sentence=\"No relevant sentence found\"\n","    for i,sentence in enumerate(corpus):\n","        similarity=len(set(query).intersection(sentence))/float(len(set(query).union(sentence)))\n","        if similarity\u003emax_similarity:\n","            max_similarity=similarity\n","            most_relevant_sentence=sentences[i]\n","    return most_relevant_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"DlCnfxU23kUG"},"outputs":[{"name":"stdout","output_type":"stream","text":["Chatbot: Ask me anything about WW2! Type 'exit' to quit\n","Chatbot: World War II began in 1939 when Germany invaded Poland\n","Chatbot: The main Allied powers were the United States, the Soviet Union, and the United Kingdom\n","Chatbot: The Axis powers included Germany, Italy, and Japan\n","Chatbot: Key battles included Stalingrad, Normandy, and Midway\n"]}],"source":["#Chatbot function to interact with the user\n","txt_file='/content/ww2 (1).txt' #Ensure this file exists in the same directory\n","def chatbot():\n","  file_path=txt_file #Ensure this file exists in the same directory\n","  sentences,corpus=load_and_preprocess_text(file_path)\n","  print(\"Chatbot: Ask me anything about WW2! Type 'exit' to quit\")\n","  while True:\n","    query=input(\"You: \")\n","    if query.lower()=='exit':\n","      print(\"Chatbot: Goodbye!\")\n","      break\n","    response=get_most_relevant_sentence(query,corpus,sentences)\n","    print(\"Chatbot:\",response)\n","\n","#Run the chatbot\n","if __name__==\"__main__\":\n","    chatbot()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMPfdD7qlr+oiQPsyeu2sHX","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}